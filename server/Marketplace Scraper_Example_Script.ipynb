{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7fd475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import wget\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from splinter import Browser\n",
    "from selenium.webdriver.firefox.service import Service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4357f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_facebook_marketplace_query(params):\n",
    "    '''Returns a URL for Facebook Marketplace given user parameters'''\n",
    "    # check for the two required parameters, if they are not specified, then exit\n",
    "    if 'location' not in params or 'item' not in params:\n",
    "        raise ValueError('location or item was not specified.')\n",
    "    \n",
    "    # make default params if they are not specified\n",
    "    if 'condition' not in params:\n",
    "        params['condition'] = []\n",
    "    \n",
    "    if 'min_price' not in params:\n",
    "        params['min_price'] = 0\n",
    "    \n",
    "    if 'max_price' not in params:\n",
    "        params['max_price'] = 1000\n",
    "    \n",
    "    if 'days_since_listed' not in params:\n",
    "        params['days_since_listed'] = 10\n",
    "\n",
    "    # lookup table for conditions \n",
    "    condition_separator = \"%2C\"\n",
    "    conditions = {0: \"new\", 1: \"used_like_new\", 2: \"used_good\", 3: \"used_fair\"}\n",
    "\n",
    "    # create the conditions part of url\n",
    "    condition = \"\" if params['condition'] == [] else \"&itemCondition=\" + conditions[params['condition'][0]]\n",
    "    for index in params['condition']:\n",
    "        condition += condition_separator + conditions[index]\n",
    "\n",
    "    # create the query string\n",
    "    base_url = f\"https://www.facebook.com/marketplace/{params['location']}/search?\"\n",
    "    url = f\"{base_url}minPrice={params['min_price']}&maxPrice={params['max_price']}&daysSinceListed={params['days_since_listed']}{condition}&query={params['item']}\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea976a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_marketplace_html(url):\n",
    "    \"\"\"Returns Facebook Marketplace Data in the format of BeautifulSoup Object\"\"\"\n",
    "    browser = Browser('firefox')\n",
    "    browser.visit(url)\n",
    "    # close pop out window\n",
    "    if browser.is_element_present_by_css('div[aria-label=\"Close\"]', wait_time=10):\n",
    "        # Click on the element once it's found\n",
    "        browser.find_by_css('div[aria-label=\"Close\"]').first.click()\n",
    "    \n",
    "    # Scroll down to load more results\n",
    "    \n",
    "    # Define the number of times to scroll the page\n",
    "    scroll_count = 4\n",
    "\n",
    "    # Define the delay (in seconds) between each scroll\n",
    "    scroll_delay = 2\n",
    "\n",
    "    # Loop to perform scrolling\n",
    "    for _ in range(scroll_count):\n",
    "        # Execute JavaScript to scroll to the bottom of the page\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        # Pause for a moment to allow the content to load\n",
    "        time.sleep(scroll_delay)\n",
    "    \n",
    "    # Parse the HTML\n",
    "    html = browser.html\n",
    "\n",
    "    # close browser window\n",
    "    time.sleep(1)\n",
    "    browser.quit()\n",
    "\n",
    "    # Create a BeautifulSoup object from the scraped HTML\n",
    "    return soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_marketplace_data(market_soup):\n",
    "    '''Returns a Pandas Data Frame with information about Facebook Marketplace listings'''\n",
    "\n",
    "    # get listings\n",
    "\n",
    "    listing_divs = market_soup.find_all('div', class_=\"x9f619 x78zum5 x1r8uery xdt5ytf x1iyjqo2 xs83m0k x1e558r4 x150jy0e x1iorvi4 xjkvuk6 xnpuxes x291uyu x1uepa24\")\n",
    "    data = []\n",
    "    for listing_div in listing_divs:\n",
    "        \n",
    "        # Extract all the necessary info and insert into lists\n",
    "        picture_div = listing_div.find('img', class_=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\")\n",
    "        if picture_div == None:\n",
    "            continue\n",
    "        picture_url = picture_div.get('src')\n",
    "\n",
    "        title_div = listing_div.find('span', class_=\"x1lliihq x6ikm8r x10wlt62 x1n2onr6\")\n",
    "        if title_div == None:\n",
    "            continue\n",
    "        title = title_div.text.strip()\n",
    "\n",
    "        price_div = listing_div.find('span', class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x676frb x1lkfr7t x1lbecb7 x1s688f xzsf02u\")\n",
    "        if price_div == None:\n",
    "            continue\n",
    "        price = price_div.text.strip()\n",
    "\n",
    "        location_div = listing_div.find('span', class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x x4zkp8e x3x7a5m x1nxh6w3 x1sibtaa xo1l8bm xi81zsa\")\n",
    "        if location_div == None:\n",
    "            continue\n",
    "        location = location_div.text.strip()\n",
    "\n",
    "        url_div = listing_div.find('a', class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1heor9g x1sur9pj xkrqix3 x1lku1pv\")\n",
    "        if url_div == None:\n",
    "            continue\n",
    "        url = \"www.facebook.com\" + url_div.get('href')\n",
    "\n",
    "        listing = {\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'location': location,\n",
    "            'image_url': picture_url,\n",
    "            'url': url\n",
    "        }\n",
    "        data.append(listing)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e217bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ebay_listings(item):\n",
    "    '''Returns a Pandas Data Frame with information about Ebay listings'''\n",
    "    ebay_url = f\"https://www.ebay.com/sch/i.html?_nkw={item}\"\n",
    "    response = requests.get(ebay_url)\n",
    "\n",
    "    ebay_soup = soup(response.text, 'html.parser')\n",
    "    \n",
    "    item_divs = ebay_soup.find_all('li', class_='s-item s-item__pl-on-bottom')\n",
    "\n",
    "    prices = [div.find('span', class_='s-item__price').text for div in item_divs]\n",
    "\n",
    "    urls = [div.find('a', class_='s-item__link').get('href') for div in item_divs]\n",
    "\n",
    "    \n",
    "    image_divs = [div.find('div', class_='s-item__image-wrapper image-treatment') for div in item_divs]\n",
    "    image_tags = [div.find('img') for div in image_divs]\n",
    "    image_urls = [url.get('src') for url in image_tags]\n",
    "\n",
    "    titles_div = [div.find('div', class_='s-item__title') for div in item_divs]\n",
    "    titles = [div.find('span').text for div in titles_div]\n",
    "\n",
    "    data = {\n",
    "        'title': titles,\n",
    "        'price': prices,\n",
    "        'image_url': image_urls,\n",
    "        'url': urls\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296541e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ebay_listings(df):\n",
    "    \"\"\"Returns a cleaned data frame without any outliers\"\"\"\n",
    "    # remove any special characters and convert to float\n",
    "    df['price'] = df['price'].replace('[\\$\\,]', '', regex=True).astype(float)\n",
    "\n",
    "    # remove the first two rows\n",
    "    df = df.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "    # Remove outliers in the 'Price' column\n",
    "    # Here we use the IQR method to identify outliers\n",
    "\n",
    "    Q1 = df['price'].quantile(0.25)\n",
    "    Q3 = df['price'].quantile(0.60)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define lower and upper bound for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filter out the outliers\n",
    "    df_cleaned = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]\n",
    "\n",
    "    return df_cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32007a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ebay_stats(df):\n",
    "    \"\"\"Returns a data frame with statistics of ebay listings\"\"\"\n",
    "    # Calculate key statistics\n",
    "    mean_price = df['price'].mean()\n",
    "    median_price = df['price'].median()\n",
    "    std_dev_price = df['price'].std()\n",
    "    fair_price = median_price * 1.05  # Adjust by 5% for current market trends\n",
    "\n",
    "    # store and return states in data frame\n",
    "    stats = {\n",
    "        'mean_price': [mean_price],\n",
    "        'median_price': [median_price],\n",
    "        'std_dev': [std_dev_price],\n",
    "        'fair_price': [fair_price]\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(stats).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110dc806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_google_lens(image_url):\n",
    "    \"\"\"Returns Google Lens Data in the format of BeautifulSoup Object\"\"\"\n",
    "    google_lens_url = f\"https://lens.google.com/uploadbyurl?url={image_url}\"\n",
    "    browser = Browser('firefox')\n",
    "    browser.visit(google_lens_url)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    # Scroll down to load more results\n",
    "    \n",
    "    # Define the number of times to scroll the page\n",
    "    scroll_count = 2\n",
    "\n",
    "    # Define the delay (in seconds) between each scroll\n",
    "    scroll_delay = 2\n",
    "\n",
    "    # Loop to perform scrolling\n",
    "    for _ in range(scroll_count):\n",
    "        # Execute JavaScript to scroll to the bottom of the page\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        # Pause for a moment to allow the content to load\n",
    "        time.sleep(scroll_delay)\n",
    "    \n",
    "    # Parse the HTML\n",
    "    html = browser.html\n",
    "\n",
    "    # close browser window\n",
    "    time.sleep(1)\n",
    "    browser.quit()\n",
    "\n",
    "    # Create a BeautifulSoup object from the scraped HTML\n",
    "    return soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05043bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the parameters\n",
    "params = {\n",
    "    'location': \"pittsburgh\",\n",
    "    'item': \"jersey\",\n",
    "    'condition': [],\n",
    "    'min_price': 0,\n",
    "    'max_price': 100,\n",
    "    'days_since_listed': 10\n",
    "}\n",
    "\n",
    "# create the facebook marketplace query\n",
    "url = create_facebook_marketplace_query(params)\n",
    "\n",
    "# Visit the website and gather data\n",
    "market_soup = fetch_marketplace_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse html to create structured listing data\n",
    "facebook_listings = parse_marketplace_data(market_soup)\n",
    "facebook_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5139e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebay_listings = get_ebay_listings(facebook_listings['title'][0])\n",
    "ebay_listings = filter_ebay_listings(ebay_listings)\n",
    "ebay_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ddd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebay_stats = calc_ebay_stats(ebay_listings)\n",
    "ebay_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1726b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_image(image_path):\n",
    "    url = \"https://lens.google.com/upload\"\n",
    "    headers = {\n",
    "        'Content-Type': 'multipart/form-data',\n",
    "    }\n",
    "\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        files = {\n",
    "            'image': (image_path, image_file, 'image/jpeg')\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers, files=files)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print('Image uploaded successfully')\n",
    "        print('Response:', response.json())\n",
    "    else:\n",
    "        print('Failed to upload image')\n",
    "        print('Status code:', response.status_code)\n",
    "        print('Response:', response.text)\n",
    "    return response.json()\n",
    "\n",
    "# Replace 'your_image.jpg' with the path to your image file\n",
    "# Replace 'your_oauth_token' with your actual OAuth token\n",
    "response = upload_image('image.jpg')\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a browser session (this example uses Chrome, but you can use Firefox by changing 'chrome' to 'firefox')\n",
    "with Browser('firefox') as browser:\n",
    "    # Open the URL where the form is located\n",
    "    url = \"https://www.google.com\"\n",
    "    browser.visit(url)\n",
    "    time.sleep(3)\n",
    "    # find the search by image button\n",
    "    image_button = browser.find_by_css('nDcEnd')\n",
    "    image_button.click()\n",
    "\n",
    "    time.sleep(1)\n",
    "    # Find the file input element and upload the image\n",
    "    file_input = browser.find_by_name('encoded_image')  # Adjust the selector based on your form\n",
    "    print(file_input)\n",
    "    \n",
    "    file_input.fill('image.jpg')\n",
    "    time.sleep(3)\n",
    "    # Optionally, fill other form fields if necessary\n",
    "    # browser.find_by_name('other_field').fill('value')\n",
    "\n",
    "    # Submit the form\n",
    "    # submit_button = browser.find_by_name('submit')  # Adjust the selector based on your form\n",
    "    # submit_button.click()\n",
    "\n",
    "    # Optionally, wait for some response or next page to load\n",
    "    browser.is_text_present('Success', wait_time=10)  # Adjust based on the expected success message\n",
    "\n",
    "    # Print the URL of the current page (can be used to verify the navigation)\n",
    "    print(browser.url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62909381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: ONLY EBAY IMAGES ARE WORKING RIGHT NOW\n",
    "# FACEBOOK IMAGES RETURN ERROR (Probably have bot auth.)\n",
    "# MAYBE PASS THEM THROUGH A LINK GENERATOR FIRST\n",
    "\n",
    "image_url = facebook_listings[\"image_url\"][0]\n",
    "print(image_url)\n",
    "# image_filename = wget.download(image_url, \"image.jpg\")\n",
    "image_url = \"https://i.imgur.com/RQC5FHC.jpeg\"\n",
    "google_lens_soup = search_google_lens(image_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('virtualenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0dea307777b2f4630875d1396a32912501fdae588fa34a4a74f40b8188c0617"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
